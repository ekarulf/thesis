\chapter{Introduction}

The field of modern robotics is advancing at an incredibly rapid pace. Steve Cousins, a noted roboticist, suggests that in the next ten to twenty years our culture will experience a personal robotics revolution much like the personal computing revolution of the 1980's and 1990's. [XX] The emergence of robots as a viable asset to, or for, daily living will certainly be an extraordinary feat. In years past, robots were often viewed by the general public as large, clumsy entities, but the emergence of present day robots defies such previous perceptions. The hardware of modern day robots possesses to have the tactile strength and finesse to grasp a light bulb without crushing it. Advances in the fields of navigation, sensors, and machine learning have produced robotic vehicles capable of autonomously navigating within urban environments.

The successful acceptance and prevalence of robots in our society requires that strides in robotics match the advances in hardware and systems software with advances in user interfaces. The study of the aforementioned field is known as Human-Robot Interaction, also known as HRI. Interestingly, HRI applies the principles of Human-Computer Interaction (HCI) to the field of robotics. Originally, HRI was restricted to keyboard and mouse interfaces; however, advances in hardware allowed roboticists a much richer selection of interfaces. A small selection of modern interfaces is illustrated in Figure ??.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=3.5in]{images/placeholder.png}
\caption{Existing 2D and 3D interfaces\label{fig:existing-robot-ui}}
\end{center}
\end{figure}

Similarly, video games represent another young, yet relevant, emerging field within the Computer Science community. Originally created as an application of computer graphics for use in the entertainment industry, video game development has matured into an independent industry in its own right. The popularity of video games has grown significantly throughout the past decade. The Entertainment Software Association estimates 68\% of American households now play computer or video games. [XX] This large demographic represents a pool of users already versed in exploring 3D virtual worlds.

Video games represent not only a large user base but they also have utility and direct applications to the fields of artificial intelligence and machine learning. Dr. Luis Von Ahn found that video games can be an effective tool for generating training data sets for machine learning. [XX] Dr. Von Ahn developed special video games he calls ``games with a purpose''.  These games provide a way to solve problems that may be considered trivial for humans, yet are very challenging for computers. \textbf{EXPAND THOUGHT / TRANSITION HERE.} For example, in Dr. Von Ahn's ``ESP Game'' humans are paired anonymously to classify images as quickly as possible.

Dr. Daniel Grollman, in his dissertation work, used video games to capture training data for robotic user interfaces. [XX] Grollman's application, RGame, recorded userâ€™s actions as they controlled a robotic dog and taught it to play soccer. Using the data in aggregate, he created an AI model for shooting and defending. \textbf{EXPAND THOUGHT / TIE INTO INTRO}

As the field of robotics continues to evolve and mature, I predict the need for training data and effective user interface design will become more pronounced and prevalent. I propose that the robotics community should apply video game design concepts to robot control and display interfaces. I hypothesize that resulting interfaces will be intuitive and easy to use for users regardless of video game experience.

In order to defend this hypothesis, I will examine prior work in the field of robotics user interfaces and present the strengths and weaknesses of the existing models. I will also identify several salient concepts from video game design and describe how these concepts would improve the existing user interfaces. Finally, I will describe the testing procedures and results of a simple environmental search user study performed on the user interface.

\chapter{Exploration of User Interfaces}
\label{chapter:ui}

Human-Computer Interaction, HCI, is the study of how humans interface with technology. This includes traditional interfaces, like graphical user interfaces controlled with a mouse and keyboard, as well as alternative interfaces such as the accelerometers in the Nintendo Wii(TM) controller.

Despite the advances in efficacy and popularity of non-traditional devices, the remainder of this paper will be limited to devices standard on most computer systems. In Section~\ref{section:futurework} I will briefly introduce where alternative interfaces would enhance the proposed interface.

\section{Robotic User Interfaces}
% From wds@

Human-Robot Interaction is a subset of Human-Computer Interaction which focuses on how humans interface with robots. In his survey paper on HRI, Dr. Michael Goodrich introduces five attributes, shown in Figure~\ref{fig:five-attributes} that define the interactions between humans and robots (Goodrich 216-217).

\begin{figure}[ht]
	\makebox[\textwidth]{\hrulefill}
	\begin{list}{$\bullet$}
		\item Level of behavior and autonomy
		\item Nature of information exchange
		\item Structure of the team
		\item Adaptation, learning, and training of people and the robot
		\item Shape of the task
	\end{list}
	\makebox[\textwidth]{\hrulefill}
	\caption{Five attributes of Human-Robot Interaction \label{fig:five-attributes}}
\end{figure}


\subsection{Autonomy}

Autonomy, independent robot behavior, is an important factor in HRI design. The relationship between a robot and a human, as provided through the interface, can be viewed or conceptualized along the lines of a continuum or a scale of autonomy. Teleoperation represents one end of the spectrum, where the robot exhibits no qualities of autonomous behavior. On the other hand, a fully autonomous robot, that ignores human input, represents the other extreme of autonomy. A simple scale of autonomy can be found in Figure~\ref{fig:autonomy}. [XX]


\begin{figure}[ht]
\begin{center}
\includegraphics[width=5in,height=1.75in]{images/placeholder.png}
\caption{Scale of Robot Autonomy\label{fig:autonomy}}
\end{center}
\end{figure}

% Awk. -ek
Robots, as they presently exist, have a fairly limited set of autonomous actions available. As a result, most interfaces, currently, are artificially conservative in their autonomy due to limitations in artificial intelligence. As research into artificial intelligence and machine learning grows, a much more diverse set of autonomous behaviors will inevitably become available. When developing robot interfaces, it is important to design them with the future levels of autonomy in mind. 

The Robot Interactive Display Environment, also known as RIDE, employs a dynamic approach known as sliding autonomy. This allows the human and the robot to change the level of autonomy as needed. RIDE currently allows the human to increase or decrease the level of autonomy on a per robot basis. In contrast, the robots are capable of only decreasing the level of autonomy. For illustration purposes, consider a robot that explores a warehouse. If the robot planned a non-optimal navigation path, a user could manually specify the waypoints. If the robot is unable to complete the specified path due to an obstruction in the path, the robot could ask the user to directly teleoperate around the obstacle. Once free of the obstruction, the user could input a new set of goal coordinates and allow the autonomous navigation system to resume control.


% Needs tightening -ek
While the RIDE application does not specify any restrictions, I have elected to restrict the scope of my thesis to allow for a more careful examination of environmental searching within the range of supervised autonomy. In Section~\ref{section:futurework} I explain how these principles of RIDE could be adapted to support more autonomous behavior.

\subsection{Information Exchange}

The nature of information exchange defines the flow of data between the human and the robot. What information is provided, how it is represented, and when it is communicated are all properties of the interface design. This encompasses low-level communication of navigation and sensor data as well as high-level commands sent from the human. 

The purpose of visualization interfaces is to represent the one-way exchange of information from the robot to humans. This data typically includes location information, laser or sonar readings, battery readings, accelerometer readings, and map data. Original interfaces were written to be accessible programatically. These interfaces were slowly adapted to display sensor data graphically, but the resulting interfaces were disjoint and required operator training. Examples of these 2D user interfaces can be seen in Figure ??.

In 2007 Dr. Curtis Nielsen introduced an ecological user interface with the goal of combining sensor data into a single, integrated display. Nielsen designed his interface to for effective teleoperation, the technique of controlling a robot remotely. In his study, Nielsen compared an integrated 3D user interface to a simple 2D user interface for several environment searching tasks. The 3D user interface displayed combined sensor data through a single viewport while the 2D user interface displayed the same data through individually. The study found the 3D interface decreased collisions and decreased task completion time.

Nielsen attributes the success of the 3D interface to improved situational awareness. The observed effect of situational awareness and context on interface effectiveness is congruous with the findings of other researchers.

A limitation of Nielsen's interface is that it's focus on teleoperation leaves the robot without autonomy. While teleoperation may be optimal for single robot environments it does not allow for concurrent robot control. This limitation requires an human operator for every robot.

We adapted several elements from the Nielsen's ecological user interface in our design of RIDE. We designed the user interface to display inside a single viewport to avoid splitting the user's attention. We also procedurally construct a 3D world using the map and sensor data. This rich user experience allows the user visually perceive the state of the world as the robot perceives it. Finally, we introduced several new concepts to improve upon the weaknesses of the existing interfaces. 

We introduced an asynchronous task system to allow the human to give the robot tasks. A robot may run only one single task at a time, and receiving a new task will replace the previous task. The task system was written generically to allow each robot to provide actions appropriate to its hardware and software. Additional details on the implementation of the task system can be found in Section ??.

To complement the task system, a notification system was also introduced. The notification system allows the robot to communicate to/with the human. The design of the notification system was engineered to enable the robot to select the importance of the message, and it permits the human to filter messages by importance. This gives the human operator the ability to ignore routine informational messages while still receiving urgent messages. An example notification is shown in Figure ??. The example shows the robot self-reporting that it is unable to reach itâ€™s goal endpoint.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=4.0in]{images/placeholder.png}
\caption{RIDE Notification Example\label{fig:ride-notification}}
\end{center}
\end{figure}

\section{RIDE User Interface}

The core of the RIDE user interface is the concept of sliding autonomy. In order to accomplish effective sliding autonomy, RIDE takes several cues from the video game industry. In our design phase, we decided to separate the user interface into two segments, the supervisor mode and the direct mode. The ``supervisor mode'' relies on robot to perform actions autonomously and report the information back to the human as appropriate. The ``direct mode'' allows the user to directly teleoperate a robot. By including both control styles, the user is given flexibility to use the most appropriate control mode for a given situation.

In addition to switching the control schemes between modes, we also change the fidelity of information exchanged. We found that providing streaming video for each robot was taxing not only on the network bandwidth but also on the human's perception. The movement from the video would draw the users eye to several places on the screen making effective management difficult. To combat this problem we disable video sensors by default in the supervisor mode. When a user wishes to inspect a region with higher fidelity or the user wishes to teleoperate the robot, switching into the direct mode enables all sensors for the current robot. This design decision seemed to be fairly intuitive as will be explained in Section ??.

In the next two subsections, I will discuss the design decisions for each user interface mode and the video game genre that inspired the interface. \textbf{I DON'T KNOW WHAT ELSE TO WRITE HERE, POSSIBLY CUSTOMIZATION?}.

% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=4.0in]{images/placeholder.png}
% \caption{Commercial video game user interfaces\label{fig:game-ui}}
% \end{center}
% \end{figure}
% 
\subsection{Direct Control}

The direct control mode sets up a high fidelity of information exchange between the human and the robot as the user teleoperates the robot. There is a fair amount of prior work in the field of interfaces for teleoperation. For that reason, video game design elements that would compliment the existing interfaces. While there are several styles of video game that we could draw on for inspiration, we decided to base our direct control mode off of ``open environment games.''

While not a genre in its own right open environment, or sandbox, games define a style of gameplay. Instead of the story following a predetermined linear path, the player is instead able to explore a world with the story unfolding around them. This style of gameplay is harolded by some as the future of games. Putting aside the narrative aspect of open environment games, the design elements that they employ have direct applicability to robotic interfaces.

The interfaces of open environment games are usually fairly minimal preferring leave most of the user interface available for camera display. The few user interface elements that do exist are typically semi-transparent to allow the user to see through UI components to the map. We applied this principal to all user interface components in RIDE by applying a small amount of alpha transparency. You can see an example of the transparency looking at the floor behind the user interface elements at the bottom of the screen in Figure ??. 

In most open environment games, a third person camera follows behind the player's avatar. While controls exist that allow the user to manually rotate the camera, most gameplay occurs with default camera positioning. This camera control style is the inspiration behind our third-person direct control mode. The user uses the arrow keys on the keyboard to move the robot in a given direction and uses the mouse to rotate the camera as needed.

A common component of most open environment games is a minimap which allows users to orient themselves with relation to the ``world''. In video games, important characters, landmarks, and other notable locations may be displayed in the minimap. We added a minimap to the lower right hand corner of the RIDE user interface. The minimap is a component common to both the direct and supervisory control interfaces and we feel strongly that the minimap's location on screen should remain in the same location for both modes.

\subsection{Supervisory Control}

The supervisory control interface is what set's RIDE apart from the interfaces described in the prior work. Without any existing user interfaces to build on top of, we went looking for genre of video game to base our design off of.

Our search led us to the genre of Real-time strategy games (RTS). RTS games are full genre in the world of computer and video games. The goal of real-time strategy games is to effectively control your units to achieve a specified goal. Often these goals require the user to control units across a large  map. Real-time strategy games stress a users ability coordinate large numbers of units in a short amount of time. 

RTS games use an isomorphic top-down view to display a large section of the environment at a time. The map can be navigated using either the keyboard or the mouse. Placing the moue pointer on the edge of the screen will translate the camera in the given direction. The keyboard can also be used directly manipulate the camera through both translation and rotation. An example user interface for a simple real time strategy can be seen in Figure ??. 

The control scheme for RTS games is a ``point and click'' style. The player begins by selecting a unit by left clicking on the unit's in the viewport. Internally the RTS game uses ray projection to determine what selectable object if any was selected. If the user is successful, the user interface will update denote a selected unit by updating the on-screen user interface components and marking the unit in the viewport. If no unit could be found any currently selected objects will become deselected.

We found that real time strategy games included their own task system similar to the task system described in Section ??. Furthermore, the realtime strategy games contained multiple different types of units. These units each had separate abilities, strengths, and weaknesses. We wanted a way to visualize this information in the user interface without cluttering the main viewport during normal use. We developed two user interface widgets to represent the abilities, strengths, and weaknesses of each robot visually. 

The first element known as the ``information panel'' can be seen in Figure ??. When a robot is selected, the information panel shows a listing of the robot's name, type, battery status, current task, and a list of sensors. A small 3D mockup of the front of the robot is also displayed to visually represent the make of the robot.

The second panel developed for the supervisory mode was the ``action panel.'' The action panel displays a list of buttons representing available actions for the selected robot. This list of actions is populated from what the robot self-reports actions it can perform. The implementation details of how actions are auto-discovered is briefly covered in Section ??. Clicking on a button will prompt for any additional arguments, such as asking the user to click on a position, and then send the task instruction to the robot. In our development we found that visualizing a confirmation helped improve clarity of what instructions were given to the robot. As an example when the user instructs a robot to move to a specific destination, the user interface will blink the target icon twice on the destination.  

Having successfully implemented a basic RTS interface, we turned back to look at additional features of real time strategy interfaces. We found that players would organize units into heterogenous groups such that the individual units complimented each other's strengths and weaknesses. We modified the existing codebase to support group selection. Instead of a single left click, the user should left click and drag a rectangle around the robots he or she wishes to select. Instead of showing detailed information on a single robot, the the ``information panel'' will be updated to show basic information on many robots. The information panel displays a 3D mockup for each robot currently selected. If the user mouses over the mockup, additional information about the robot appears.

In addition to the information panel, the ``action panel'' also has a slight change in behavior. The action panel selects the union of set of actions available to each robot. RTS games provided a base set of actions that all controllable units must be able to perform, ``Stop'' and ``Move''. We require the same two actions to be available for all robots in RIDE. The advantage of requiring the move command for all robots is that we can use a shortcut found in most RTS games. When a robot is selected -- instead of click on the move button and then clicking on a destination, the user need only right click on the destination on the ground.

% Need more

% RTS view
% Higher concurrency / lower fidelity
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=3.5in]{images/placeholder.png}
% \caption{RIDE Supervisory Control\label{fig:ride-ui-super}}
% \end{center}
% \end{figure}
% 
% 
% % FPS view
% % Single concurrency / high fidelity
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=3.5in]{images/placeholder.png}
% \caption{RIDE Direct Control\label{fig:ride-ui-direct}}
% \end{center}
% \end{figure}
