%!TEX root = karulf-thesis.tex
\chapter{Implementation}

We present RIDE, the Robot Interactive Display Environment, as an example of a multi-mode interface. While a number of control interfaces either similar to, or draw direct inspiration from, RTS games \cite{Bruemmer05turnoff} \cite{Kadous} \cite{Mclurkin} \cite{Tejada} or from first or third-person games \cite{Bruemmer05turnoff} \cite{Humphrey} \cite{Maxwell}, we are not aware of any systems which provide as many RTS interface features as RIDE. We are also not aware of any existing interfaces that allow the user to move between task-level and direct control in the way that RIDE allows. We should also note that the work presented in this paper builds directly on a previous prototype version of RIDE \cite{rrsd08}. We discuss the user interface in full detail in Section~\ref{sec:ride_user_interface}.

We use a robotics middleware package, ROS, to abstract details of robotics software from the RIDE user interface. We briefly introduce key concepts of ROS, the Robot Operating System, in Section~\ref{sec:ros}. In Section~\ref{sec:ride_code}, we describe the system level architecture of RIDE. The source code for RIDE is freely available at the Washington University ROS repository, \verb!http://wu-ros-pkg.sourceforge.net!.

%% Unused!
% RIDE allows the human to increase or decrease the level of autonomy on a per robot basis. In contrast, the robots are capable of only decreasing the level of autonomy.

% Needs tightening -ek
%While the RIDE application does not specify any restrictions, wehave elected to restrict the scope of my thesis to allow for a more careful examination of environmental searching within the range of supervised autonomy. In Section~\ref{section:futurework} weexplain how these principles of RIDE could be adapted to support more autonomous behavior.

% When porting the user interface from RTS games to human-robot interaction, we were able to re-use many of the existing design elements: unit selection, grouping, the action system, and minimap. We also found that we needed to display additional elements: sensor visualization, coverage maps


\section{User Interface}
\label{sec:ride_user_interface}

RIDE provides multiple interface modes, each specialized for different levels of autonomy. In our design phase, we decided to separate the user interface into two segments: the supervisor mode and the direct mode. The ``supervisor mode'' relies on the robot to perform actions autonomously and report the information back to the human, as appropriate. The ``direct mode'' allows the user to directly teleoperate a robot. By including both control styles, the user is given flexibility to use the most appropriate control mode for a given situation.

In addition to switching the control schemes between modes, we also changed the fidelity of information exchanged. We found that by providing streaming video for each robot, it was not only taxing on the network bandwidth, but also wearing on the human's perception. The movement from the video would draw the users eye to several places on the screen making effective management difficult. To combat this problem, we reduced the number of available sensors in the supervisor mode. When a user wished to inspect a region with higher fidelity, he could switch into the direct mode and enable all sensors for the current robot.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=6.10in]{images/ride-ui.pdf}
\caption{RIDE User Interface: Supervisory Mode\label{fig:ride-ui}}
\end{center}
\end{figure}

The screenshot figure in Figure~\ref{fig:ride-ui} shows the RIDE supervisory control mode with annotated descriptions of the user interface elements. The viewport is the main portion of the display. The viewport changes camera perspective based on the control mode; it is either locked behind the robot in the direct control mode or user operated in the supervisory interface. We took care to add alpha transparency to all user interface elements to prevent obstructing the viewport.

The menu bar, at the top of the screen, remains static between the two control modes. The various menus allow for customization of the user-interface on a global and per-robot basis. These settings can be saved, and restored in future sessions. There are also menu bar buttons to control the camera viewpoint and zoom level, toggle the sensor information displayed, and to list all currently known robots.

The widget panel is displayed at the bottom of the screen. The widget panel always contains a left, center, and right widget. A widget to change the display mode is always locked to the far right of the widget bar. Table~\ref{tab:ui-widgets} describes the widgets displayed based on the state of the interface.

\begin{table}[ht]
\label{tab:ui-widgets}
\begin{center}
    \begin{tabular}{ | p{4cm} | l | l | l |}
    \hline
    \textbf{Mode} & \textbf{Left Widget} & \textbf{Center Widget} & \textbf{Right Widget} \\ \hline
    Direct Control & minimap & Information Panel & Proximity Panel \\ \hline
    Supervisory Control (Single Robot) & minimap & Status Panel & Action Panel \\ \hline
    Supervisory Control (Multiple Robots) & minimap & Group Panel & Action Panel \\ \hline
    \hline
    \end{tabular}
    \caption{RIDE User Interface Widgets}
\end{center}
\refstepcounter{table}
\end{table}

\subsection{Supervisory Control}
\label{subs:ui-supervisor}

As mentioned in Section~\ref{sub:RTS_games}, RTS games employ a top-down view of the world to control many units. Real time strategy games exemplify the qualities we desire for a supervisory interface. Real-time strategy games support the task-based control of heterogenous unit types; each unit type has separate abilities, strengths, and weaknesses. We wanted a way to visualize this information in the user interface without cluttering the main viewport during normal use. We developed two user interface widgets to represent the abilities, strengths, and weaknesses of each robot visually.

The first element, known as the ``information panel,'' can be seen in Figure~\ref{fig:ride-ui}. When a robot is selected, the information panel shows a listing of the robot's name, type, battery status, current task, and a list of sensors. An iconic rendering of the robot is displayed to visually represent the model of the robot.

The second panel developed for the supervisory mode was the ``action panel.'' The action panel displays a list of buttons representing available actions for the selected robot. This list of actions is populated from what actions the robot self-reports it can perform. The implementation details of action auto-discovery is described in Section~\ref{sec:ride_code}. Clicking on a button prompts for any additional arguments, such as asking the user to click on a position; it then sends the task instruction to the robot. In our development, we found that visualizing a confirmation helped improve clarity of what instructions were given to the robot. As an example, when the user instructs a robot to move to a specific destination, the user interface will blink the target icon, twice, on the destination.  

\subsubsection{Grouping Robot Teams} % (fold)
\label{ssub:grouping_robot_teams}
In our study of RTS games, we found that players organized units into heterogenous groups such that the individual units complemented each other's strengths and weaknesses. This required an interface that allowed a user to select multiple robots, at the same time, and then control the group effectively.

When a user wished to select a group of robots, instead of a single left click, the user left click and dragged a rectangle around the robots she wished to select. Much like single unit selection, a detailed information panel displays relevant data about the robot. However, instead of showing detailed information on a single robot, the ``information panel'' will be updated to show basic information on the group as a whole. The information panel displays a 3D mockup for each robot currently selected and the name of the robot. The user may move the mouse on top of the 3D mockup to obtain additional information about the robot.

In addition to the information panel, the ``action panel'' also has a slight change in behavior. The action panel selects the intersection of a set of actions available to each robot. RTS games provided a base set of actions that all controllable units must be able to perform: ``Stop'' and ``Move.'' We require the same two actions be defined for all robots in RIDE. The advantage of requiring the move command for all robots is that we could utilize a shortcut found in most RTS games. When a robot is selected -- instead of clicking on the move button and then clicking on a destination, the user need only right click on the destination location.
% subsubsection grouping_robot_teams (end)

% Need more

% RTS view
% Higher concurrency / lower fidelity
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=3.5in]{images/placeholder.png}
% \caption{RIDE Supervisory Control\label{fig:ride-ui-super}}
% \end{center}
% \end{figure}
% 
% 
% % FPS view
% % Single concurrency / high fidelity
% 
% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=3.5in]{images/placeholder.png}
% \caption{RIDE Direct Control\label{fig:ride-ui-direct}}
% \end{center}
% \end{figure}

\subsection{Direct Control}

The direct control mode sets up a high fidelity of information exchanged between the human and the robot as the user teleoperates the robot. Due to the large amount of prior work in the field of interfaces for teleoperation, we searched for video game design elements that would complement the existing interfaces. While there are several styles of video games that we could draw from for inspiration, we decided to base our direct control mode on ``open environment games.''

While not a genre in its own right, open environment games, or sandbox games, define a style of gameplay. Rather than the story following a predetermined linear path, the player explores a world where the story unfolds around her. For some, this style of gameplay is heralded as the future of games. Setting aside the narrative aspect of open environment games, the design elements that they employ have direct applicability to robot interfaces.

The interfaces of open environment games are usually fairly minimal; the preference is to leave most of the user interface available for camera display. The few user interface elements that do exist are typically semi-transparent, which allows the user to see through UI components to the map. We applied this principle to all user interface components in RIDE by applying a small amount of alpha transparency. You can see an example of the transparency looking at the floor behind the user interface elements, at the bottom of the screen, in Figure~\ref{fig:ride-ui}.

In most open environment games, a third person camera follows behind the player's avatar. While controls exist that allow the user to manually rotate the camera, most gameplay occurs with default camera positioning. This camera control style is the inspiration behind our third-person direct control mode. The user uses the arrow keys, on the keyboard, to move the robot in a given direction and uses the mouse to rotate the camera as needed.

A common component of most open environment games is a minimap which allows users to orient themselves, with relation to the ``world.'' In video games, important characters, landmarks, and other notable locations may be displayed in the minimap. We added a minimap to the lower right hand corner of the RIDE user interface. The minimap is a component common to both the direct and supervisory control interfaces. We feel strongly that the minimap's location on screen should remain in the same location for both modes.

\subsection{Tasks and Notifications}
We adapted several elements from Nielsen's ecological user interface in our design of RIDE. We arranged the user interface to display inside a single viewport, to avoid splitting the user's attention. We also procedurally constructed a 3D world using the map and sensor data. This rich user experience allows to the user visually perceive the state of the world, as the robot perceives it. Finally, we introduced new concepts to improve upon the weaknesses of the existing interfaces: a task system and a notification system \cite{Nielsen_Teleoperation}.

We introduced an asynchronous task system, which allows the human to directly command robots through a set of predetermined tasks. A robot may run only one single task at a time, and receipt of a new task replaces the previous task. The task system was written generically to allow each robot to provide actions appropriate to its hardware and software. Additional details on the implementation of the task system can be found in Section~\ref{subs:ui-supervisor}.

To complement the task system, a notification system was also introduced. The notification system enables the robot to communicate with the human. The design of the notification system enables the robot to select the importance of the message, and it permits the human to filter messages by importance. This gives the human operator the ability to ignore routine informational messages, while still receiving urgent messages. An example notification is shown in Figure~\ref{fig:ride-notification}. The example shows the robot self-reporting it has discovered a box, it's goal in this search and rescue scenario.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=4in]{images/ride-notification.png}
\caption{RIDE Notification Example\label{fig:ride-notification}}
\end{center}
\end{figure}

% \subsection{Prior Work} % (fold)
% \label{sub:hri_prior_work}
% A number of control interfaces either similar to, or draw direct inspiration from, RTS games [??, ??, ??, ??] or from first or third-person games [??, ??, ??]. However, we are not aware of any systems which provide as many RTS interface features as RIDE. We are also not aware of any existing interfaces that allow the user to move between task-level and direct control in the way that RIDE allows. We should also note that the work presented in this paper builds directly on a previous prototype version of RIDE [??].
% 
% Jones and Snyder [??] describe a system that is very similar in spirit to ours, although it is designed for the control of a small number of free-flying space robots. This interface is most similar to the main display in RIDE, but it lacks other features, such as sensor visualization, the information panel, the minimap, and notifications.
% 
% Parasuraman, Galster, and Miller describe a task-level control interface called Playbook [??], and evaluate itâ€™s effectiveness on a task where subjects controlled six simulated unmanned vehicle under a range of conditions. Their interface had a 2d representation of the world, and limited sensor visualizations.
% 
% Nielsen and Goodrich [??] compared a control interface that is very similar to the main display of RIDE to one that presented a direct video feed from the robot alongside a 2d map. They concluded that the third-person display, with the video displayed as if on a projection screen in front of the robot, allowed for easier control of the robot. Our use of this technique for video display in third-person mode is motivated by the results of this work.
% 
% Richter and Drury [??] surveyed a number of video game interfaces, and characterized the various recurring elements within them. They proposed a Video-Game Based Framework (VGBF) for characterizing HRI interfaces by the input and output devices and methods, and by their input classifications.

% subsection prior_work (end)

\section{Robot Operating System}
\label{sec:ros}
The RIDE interface uses the ROS robotics middleware framework to abstract common components of robotics software. ROS breaks a large robotic system into many discrete computational \emph{nodes}. Because the nodes for a single robot operate as distinct processes, potentially across multiple platforms, ROS provides communication middleware. The communication between nodes is brokered through a special \verb!roscore! node. This \verb!roscore! node maintains a list of active nodes, topics, services, and configuration values \cite{ROS09}. We describe the publish/subscribe \emph{topic} system and the synchronous \emph{service} system in Section~\ref{sub:ros_net}. Finally, we discuss \verb!tf!, ROS's built-in kinematic tree in Section~\ref{sub:ros_tf}.

\subsection{Topics and Services}
\label{sub:ros_net}
ROS provides a framework where computational \emph{nodes} communicate via \emph{topics} and \emph{services}. A topic is a named broadcast endpoint that supports multiple readers and writers. Nodes that write to a topic are known as ``publishers.'' A publisher announces it is publishing to one or more topics to the \verb!roscore! node. Nodes that wish to read from a topic are known as ``subscribers.'' While similar in design to the Common Object Request Broker Architecture (CORBA), the \verb!roscore! node only provides a list of publishers to a subscriber node; the subscriber communicates directly with the publisher. In general, nodes either publish sensor data, transform and republish data, or control actuators from published data. A simple example of a ROS digraph can be seen in Figure~\ref{fig:middleware-ros}. While topics are well suited for periodic broadcast messages, the ``best effort'' delivery make topics unsuitable for command data \cite{ROS09}.

The \emph{services} system within ROS addresses the need for ``reliable'' message delivery. A service is a named endpoint with one \emph{service provider} and many \emph{service clients}.  Similar to topic subscribers and publishers, service providers and clients announce their intentions to the \verb!roscore! node. Unlike broadcast topics, messages pass between the service provider and client through a unicast stream. 

\begin{figure}[ht]
\includegraphics{images/middleware-ros.pdf}
\caption{Example ROS nodes and topics\label{fig:middleware-ros}}
\end{figure}

\subsection{Kinematic Tree}
\label{sub:ros_tf}
A large part of the complexity of robotic systems is transforming coordinate frames in a timely manner. ROS offers a special topic located at \verb!tf! and provides a cross-language library, also called ``tf'', to ease coordinate transformation tasks.

ROS builds a kinematic tree using data published to the \verb!tf! topic. Coordinate frames contain a timestamp, an object's name, the parent coordinate frame and the quaternion offset between the two. The tf package allows a developer to request a quaternion offset between two coordinate frames. 

In order to maintain a correct representation, the library automatically discards stale coordinate frame data. This requires objects to publish their location at a specified frequency. When all nodes are published at an acceptable rate the full kinematic tree is accessible. A sample kinematic tree for a robot with a stero-vision rig mounted on-top of a pan-tilt unit can be seen in Figure~\ref{fig:tf-example}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=3.5in]{images/tf-example.pdf}
\caption{Sample Kinematic Tree\label{fig:tf-example}}
\end{center}
\end{figure}

\section{RIDE Architecture}
\label{sec:ride_code}
Originally designed for a single robot system, ROS is not well suited to control multi-robot teams. This conflicts with RIDE's design to control large numbers of robots. We decided to maintain the ROS architecture by maintaining a \verb!roscore! and set of nodes on each robot. We define a collection \verb!roscore! and supporting nodes as a ``realm.'' We developed a utility, \verb!rosmultimaster!, to allow a single node to connect to multiple realms. This design allows robots to operate independently and gives RIDE the freedom to connect to robot realms as needed.

Each robot realm runs a RIDE node (\verb!ride_agent! in figure~\ref{fig:ride-simulation-realm}), which publishes information about the robot, including the tasks that it can perform, the sensors that it has, diagnostic information, and notifications. The RIDE interface uses this information to subscribe to relevant sensor, diagnostic, and notification topics from the robot.

Figure~\ref{fig:ride-simulation-realm} shows the set of ROS nodes for a simulated robot. All sensor information (\verb!base_scan!, \verb!odom!) originates from the simulator node (\verb!state_bridge!). Similarly, all movement commands (\verb!cmd_vel!) are consumed by \verb!state_bridge!. On a real robot, there would be separate nodes for each of the sensors, and for the movement controller.

RIDE also establishes direct, service connections with the nodes responsible for performing the tasks that can be allocated to the robot. In Figure~\ref{fig:ride-simulation-realm}, \verb!GoToNode! is responsible for movements tasks given to the robot. The \verb!ride_agent! node is also capable of sending movement commands directly to the motion controller (or simulator), to stop the robot when needed.

When robots connect to the RIDE interface, the tasks that they can perform are recorded, and this information is used to populate the task list when the robots are subsequently selected. Currently, the set of tasks that a robot can perform is assumed to be static, and drawn from a fixed set of known tasks. This assumption allows us to assign an appropriate icon for each task, and to know the parameters that each task requires (which require additional modal interactions with the interface). Similarly, we assume that the sensors available to our robots are drawn from a known set. This lets us be sure that we have appropriate visualizations for each possible sensor. We discuss plans to relax these assumptions in future work (see Section~\ref{section:futurework}).

\begin{sidewaysfigure}[ht]
\includegraphics[width=\textwidth]{images/ride-simulation-realm.png}
\caption{Publish/Subscriber graph in RIDE Simulation Realm\label{fig:ride-simulation-realm}}
\end{sidewaysfigure}


% \subsection{Simulation Realm}
% \label{sub:simulator}
% Weused a simulation environment, called ``rosstage'', to provide a consistent environment. The simulated system architecture is a superset of the regular RIDE architecture with the map realm acting as a full simulation realm and several additional topics being shared to the robot realms. An overview of the simulation architecture can be found in Figure~\ref{fig:ride-simulation-realm}.
% 
% 
% \subsection{Robot Realm}
% \label{section:robot-architecture}
% RIDE uses a modular structure for controlling robots. Due to the topic system in ROS, this functionality can be written to work regardless of if the robot is simulated or physical. Each robot must provide position data for basic RIDE functionality. Beyond position data, the user can choose what sensors, actuators, and ``actions'' to provide.
% 
% Figure~\ref{fig:ride-robot-realm} demonstrates a simulated robot realm and the components to the most basic functionality. The robot must be able to orient itself with respect to the \verb!/map! coordinate frame. This typically requires a localization node like the Adaptive Monte-Carlo Localization (amcl) node when running on a physical robot or a fake localization node when running in simulation.
% 
% In addition to providing position data, the robot must expose a path planning node. ROS provides a navigation stack complete with a path planning node, called move\_base. Lastly, the robot must provide a way to directly control the movement of the robot allowing the user to ``drive'' a robot around freely.

% Discuss ride_agent
% Discuss GoToNode

\section{Panda3D}

RIDE uses the popular 3D rendering engine Panda3D to display the virtual environment to the user. Like most rendering engines, Panda3D relies on a scene graph for rendering objects onto the screen. We dynamically constructed the 3D environment based on data from the mapping realm and each robot's representation of the world. Due to the common mapping realm, mentioned in Section~\ref{sec:ride_code}, all robots share a common coordinate frame. This allows the RIDE UI to represent a one-to-one mapping of objects from each robot's kinematic tree to objects in Panda3D's scene graph. For example, the robot given in the Figure~\ref{fig:tf-example} would render a robot equipped with a laser rangefinder and a two web cams mounted to a pan-tilt unit.

% \begin{figure}[ht]
% \begin{center}
% \includegraphics[width=5.10in]{images/ride-architecture.pdf}
% \caption{RIDE architecture: two robot realms with the shared map realm \label{fig:ride-architecture}}
% \end{center}
% \end{figure}
% 
% Panda3D provides its own run loop for running tasks, processing events and updating the scene graph. For performance reasons, Panda3D does not use the native Python threading implementation so wechose to use the synchronous API offered by rosmultimaster. Wethen added one background task per rosmultimaster instance to Panda3D's task manager. The rosmultimaster tasks were called at a specified frequency designed to keep latency low and framerate high.
